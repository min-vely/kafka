import json
import re
from typing import Any, Dict, List
from langchain_upstage import ChatUpstage

# [C1] ê°™ì€ citation ë§ˆì»¤ ì œê±°
_CIT_RE = re.compile(r"\[C\d+\]")

def strip_citations(text: str) -> str:
    return _CIT_RE.sub("", text or "").strip()


def _extract_summary_text(x: str, max_len: int = 900) -> str:
    """
    ìš”ì•½ ë¬¸ìì—´ì—ì„œ í‰ê°€ì— ë°©í•´ë˜ëŠ” ê²ƒë“¤ì„ ì œê±°í•˜ê³  'ìˆœìˆ˜ ìš”ì•½ë¬¸'ë§Œ ë‚¨ê¹€.
    - JSONì´ë©´ Summary/summary/ìš”ì•½ í‚¤ ìš°ì„  ì¶”ì¶œ
    - [ìˆ˜ì • í›„]/[ìµœì¢…]/[ê²€ì¦]/[ìµœì¢… ì¶œë ¥]/Rules/====== ê°™ì€ ë©”íƒ€ ë¸”ë¡ ì œê±°
    - citation ë§ˆì»¤([C#]) ì œê±°
    """
    s = (x or "").strip()

    # 1) JSON í˜•íƒœë©´ ìš”ì•½ í•„ë“œ ìš°ì„  ì¶”ì¶œ
    try:
        j = json.loads(s)
        if isinstance(j, dict):
            s = (
                j.get("Summary")
                or j.get("summary")
                or j.get("ìš”ì•½")
                or j.get("result")
                or s
            )
        elif isinstance(j, str):
            s = j
    except Exception:
        pass

    s = (s or "").strip()

    # 2) ë©”íƒ€/ë¡œê·¸ê°€ ë¶™ëŠ” ì¼€ì´ìŠ¤ ì»·
    cut_markers = [
        "[ìˆ˜ì • í›„]",
        "[ìµœì¢…]",
        "[ê²€ ì¦]",
        "[ê²€ì¦]",
        "[ìµœì¢… ì¶œë ¥]",
        "ìµœì¢… ì¶œë ¥",
        "Rules:",
        "RULES:",
        "==========",
    ]
    for m in cut_markers:
        if m in s:
            s = s.split(m, 1)[0].strip()

    # 3) citation ì œê±°
    s = strip_citations(s)

    # 4) ê³µë°± ì •ë¦¬
    s = re.sub(r"\s+", " ", s).strip()

    # 5) ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‰ê°€ ì•ˆì •í™”
    if len(s) > max_len:
        s = s[:max_len].rstrip()

    return s


def _safe_json(text: str) -> Dict[str, Any]:
    text = (text or "").strip()
    try:
        return json.loads(text)
    except Exception:
        m = re.search(r"\{.*\}", text, flags=re.DOTALL)
        if not m:
            raise ValueError(f"Judge did not return JSON:\n{text[:400]}")
        return json.loads(m.group(0))


def _jaccard_similarity(a: str, b: str) -> float:
    """A/Bê°€ ë„ˆë¬´ ë¹„ìŠ·í•´ì„œ íƒ€ì´ê°€ ë‚  ê°€ëŠ¥ì„± ì§„ë‹¨(ê°„ë‹¨ í† í° ìì¹´ë“œ)."""
    a_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", (a or "").lower()))
    b_tokens = set(re.findall(r"[ê°€-í£A-Za-z0-9]+", (b or "").lower()))
    if not a_tokens and not b_tokens:
        return 1.0
    if not a_tokens or not b_tokens:
        return 0.0
    inter = len(a_tokens & b_tokens)
    union = len(a_tokens | b_tokens)
    return inter / union if union else 0.0


PAIRWISE_PROMPT = """\
You are an expert evaluator for summary quality.

You will compare two summaries of the SAME article:

- A = LLM-only draft summary (no RAG)
- B = RAG-verified summary (citations removed for fair readability/coverage)

Score each on a 0-10 scale for:
1) faithfulness (supported by the article)
2) coverage (captures major points)
3) readability (clarity, conciseness)

Then provide an overall winner.

Return STRICT JSON:
{{
  "faithfulness": {{"a": <0-10>, "b": <0-10>, "winner": "A"|"B"|"TIE", "reason": "<short>" }},
  "coverage":     {{"a": <0-10>, "b": <0-10>, "winner": "A"|"B"|"TIE", "reason": "<short>" }},
  "readability":  {{"a": <0-10>, "b": <0-10>, "winner": "A"|"B"|"TIE", "reason": "<short>" }},
  "overall":      {{"a": <0-10>, "b": <0-10>, "winner": "A"|"B"|"TIE", "reason": "<short>" }},
  "notes": ["<actionable note>", "<actionable note>"]
}}

ARTICLE:
{article}

SUMMARY A (LLM draft):
{a}

SUMMARY B (RAG verified, citations removed):
{b}
"""


def eval_rag_vs_llm(
    llm: ChatUpstage,
    article_text: str,
    draft_summary: str,
    rag_summary: str,
) -> Dict[str, Any]:
    # âœ… í‰ê°€ ì „ì— A/Bë¥¼ ë°˜ë“œì‹œ "ìš”ì•½ë¬¸ë§Œ" ë‚¨ê¸°ë„ë¡ ì •ê·œí™”
    a_text = _extract_summary_text(draft_summary)
    b_text = _extract_summary_text(rag_summary)

    # BëŠ” ì›ì¹™ëŒ€ë¡œ citations ì œê±°ëœ ë²„ì „ìœ¼ë¡œ í‰ê°€ (ì´ë¯¸ ì œê±°í–ˆì§€ë§Œ í•œë²ˆ ë” ì•ˆì „í•˜ê²Œ)
    b_text = strip_citations(b_text)

    prompt = PAIRWISE_PROMPT.format(
        article=(article_text or "").strip(),
        a=a_text,
        b=b_text,
    )

    resp = llm.invoke(prompt)
    content = getattr(resp, "content", str(resp))
    result = _safe_json(content)

    # ğŸ” ì§„ë‹¨ ì •ë³´(íƒ€ì´ ë‚¨ë°œ ì›ì¸) ì¶”ê°€: A/B ìœ ì‚¬ë„
    try:
        sim = _jaccard_similarity(a_text, b_text)
        notes: List[str] = result.get("notes") or []
        if sim >= 0.85:
            notes.append(f"A and B are very similar (token Jaccard ~ {sim:.2f}). TIE is likely; consider comparing LLM-only vs RAG-generated (not RAG-verified).")
        result["notes"] = notes
        result["_debug"] = {
            "a_preview": a_text[:140],
            "b_preview": b_text[:140],
            "similarity_jaccard": round(sim, 3),
        }
    except Exception:
        pass

    return result
