# agent/nodes/nodes.py
import os
import json
import re
from typing import Any, Dict
from dotenv import load_dotenv
from langchain_upstage import ChatUpstage
from langchain_core.tools import tool
from pydantic import TypeAdapter, HttpUrl

try:
    from tavily import TavilyClient
except ImportError:
    TavilyClient = None

from agent.prompts import (
    SAFETY_PROMPT, #extract_content ë…¸ë“œì—ì„œ ì½˜í…ì¸  ì•ˆì „ë„ ê²€ì‚¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    SUMMARY_DRAFT_PROMPT,
    QUIZ_FROM_SUMMARY_PROMPT,
    JUDGE_PROMPT,
    IMPROVE_DRAFT_PROMPT,
    CLASSIFY_PROMPT,
    THOUGHT_QUESTION_PROMPT,
    KNOWLEDGE_TYPE_CLASSIFY_PROMPT,
    TAVILY_QUERY_GENERATOR_PROMPT,
    UPDATE_ANALYSIS_PROMPT,
    PERSONA_DEFINITIONS,
    PERSONA_APPLY_PROMPT,
)
#input_urlë…¸ë“œ, extract_contentë…¸ë“œ ì¶”ê°€í•˜ë©´ì„œ ìœ í‹¸ë¦¬í‹° ëª©ë¡ ìˆ˜ì •
from agent.utils import (
    is_valid_url,
    is_youtube_url,
    extract_youtube_video_id,
    get_youtube_transcript,
    get_article_content,
    calculate_ebbinghaus_dates
)
from agent.rag import verify_summary_with_rag
from agent.database import get_db

load_dotenv()

# -----------------------------
# Tools
# -----------------------------
@tool
def get_latest_update_analysis(summary_text: str) -> str:
    """
    ì£¼ì–´ì§„ ìš”ì•½(summary_text)ì— ëŒ€í•´ ìµœì‹  ì •ë³´ë¥¼ ì›¹ì—ì„œ ê²€ìƒ‰í•˜ê³ , 
    ê³¼ê±° ì •ë³´ì™€ í˜„ì¬ ìƒí™©ì„ ë¹„êµ ë¶„ì„í•œ í•œ ì¤„ ì†Œì‹ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ìµœì‹  íŠ¸ë Œë“œ, ë‰´ìŠ¤, ì¸ë¬¼ í˜„í™© ë“±ì˜ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        tavily_key = os.environ.get("TAVILY_API_KEY")
        if not (tavily_key and TavilyClient):
            return "Tavily API Keyê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
        client = TavilyClient(api_key=tavily_key)
        
        # 1. ìµœì‹  ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•œ ì „ìš© ê²€ìƒ‰ì–´ ìƒì„±
        print("   - ì „ìš© ê²€ìƒ‰ì–´ ìƒì„± ì¤‘...")
        query_gen_prompt = TAVILY_QUERY_GENERATOR_PROMPT.format(summary_text=summary_text)
        search_query_resp = llm.invoke(query_gen_prompt)
        search_query = (search_query_resp.content or "").strip()
        print(f"   - ê²€ìƒ‰ì–´: {search_query}")
        
        # 2. Tavily ê²€ìƒ‰
        print("   - Tavily ì›¹ ê²€ìƒ‰ ì¤‘...")
        response = client.search(query=search_query, search_depth="advanced", max_results=3)
        results = response.get("results", [])
        
        if not results:
            return "ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•´ ë³´ì•˜ìœ¼ë‚˜, í˜„ì¬ë¡œì„œëŠ” ì—…ë°ì´íŠ¸ëœ ë‚´ìš©ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            
        search_results_text = ""
        for res in results:
            search_results_text += f"- ì œëª©: {res['title']}\n  ë‚´ìš©: {res['content']}\n  URL: {res['url']}\n\n"
        
        # 3. ë¶„ì„
        print("   - ê²€ìƒ‰ ê²°ê³¼ì™€ ì›ë¬¸ ë¹„êµ ë¶„ì„ ì¤‘...")
        analysis_prompt = UPDATE_ANALYSIS_PROMPT.format(
            summary_text=summary_text,
            search_results=search_results_text
        )
        analysis_resp = llm.invoke(analysis_prompt)
        return (analysis_resp.content or "").strip()
        
    except Exception as e:
        return f"(ì›¹ ì„œì¹˜ ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)})"


# -----------------------------
# LLM
# -----------------------------
llm = ChatUpstage(
    model=os.getenv("KAFKA_MODEL", "solar-pro2"),
    temperature=0.2,
    api_key=os.environ["UPSTAGE_API_KEY"],
)


# -----------------------------
# Nodes
# -----------------------------
def input_url_node(state):
    """1) ì…ë ¥ì´ URLì´ë©´ ê²€ì¦í•˜ê³ , í…ìŠ¤íŠ¸ë©´ ê·¸ëŒ€ë¡œ í†µê³¼ì‹œí‚¤ëŠ” ì§€ëŠ¥í˜• ë…¸ë“œ"""
    # mainì—ì„œ ë˜ì ¸ì¤€ 'user_input'ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    user_input = state.get("user_input", "").strip()

    # www.ë¡œ ì‹œì‘í•˜ë©´ ì•ì— https:// ë¥¼ ë¶™ì—¬ì„œ URLë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•´ì•¼ ì•„ë˜ 1ë²ˆ IFë¬¸(http ì‹œì‘ ì²´í¬)ì— ê±¸ë¦½ë‹ˆë‹¤!
    if user_input.startswith("www."):
        user_input = "https://" + user_input

    # 1. URL í˜•íƒœì¸ì§€ í™•ì¸
    if user_input.startswith(("http://", "https://")):
        if is_valid_url(user_input):
            # ìœ íš¨í•œ URLì¸ ê²½ìš°
            return {
                "url": user_input,
                "is_valid": True,
                "messages": "URL í™•ì¸ ì™„ë£Œ! ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŸ¬ ê°‘ë‹ˆë‹¤."
            }
        else:
            # URL í˜•íƒœì¸ë° ê°€ì§œì¸ ê²½ìš° (ì§„ì§œ ì—ëŸ¬)
            return {
                "is_valid": False,
                "messages": "ìœ íš¨í•˜ì§€ ì•Šì€ URL í˜•ì‹ì…ë‹ˆë‹¤. ì£¼ì†Œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
            }

    # 2. URLì´ ì•„ë‹ˆê³  ì§„ì§œ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
    elif len(user_input) > 0:
        # ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš° (í†µê³¼!)
        # input_text ì¹¸ì´ ë¹„ì–´ìˆì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì—¬ê¸°ì— ì±„ì›Œì¤ë‹ˆë‹¤.
        return {
            "input_text": user_input,
            "is_valid": True,
            "messages": "í…ìŠ¤íŠ¸ ì…ë ¥ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì¶”ì¶œ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³  ë¶„ì„í•©ë‹ˆë‹¤."
        }

    # 3. ì•„ë¬´ê²ƒë„ ì•ˆ ë“¤ì–´ì˜¨ ê²½ìš°
    return {
        "is_valid": False,
        "messages": "ì…ë ¥ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
    }

def extract_content_node(state):
    """
    2) ì½˜í…ì¸  í™•ë³´ ë° LLM ìœ í•´ì„± ê²€ì¦ ë…¸ë“œ

    ì§„í–‰ ë‹¨ê³„:
    1. ë°ì´í„° í™•ì¸:
       - URLì´ ìˆìœ¼ë©´ YouTube/ì•„í‹°í´ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ
       - ì´ë¯¸ input_textê°€ ìˆìœ¼ë©´ ì¶”ì¶œ ë‹¨ê³„ ê±´ë„ˆëœ€ (ì§ì ‘ ì…ë ¥/íŒŒì¼ ëŒ€ì‘)
    2. ì½˜í…ì¸  ê²€ì¦:
       - ì¶”ì¶œë˜ê±°ë‚˜ ì…ë ¥ëœ í…ìŠ¤íŠ¸ ìƒìœ„ 2,000ìë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM Safety ê²€ì‚¬ ìˆ˜í–‰
    3. ìƒíƒœ ì—…ë°ì´íŠ¸:
       - ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ is_safe í”Œë˜ê·¸ ì„¤ì • ë° ìµœì¢… ë³¸ë¬¸ ì €ì¥
    """
    url = state.get("url")
    content = state.get("input_text", "").strip()

    # 1. ì´ë¯¸ ë³¸ë¬¸ì´ ìˆë‹¤ë©´ (ì§ì ‘ ì…ë ¥ or íŒŒì¼) ì¶”ì¶œ ê±´ë„ˆë›°ê¸°
    if content and not url:
        print("ì´ë¯¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì¶”ì¶œ ë‹¨ê³„ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    # 2. URLì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ì¶œ ì‹¤í–‰
    elif url:
        try:
            if is_youtube_url(url):
                video_id = extract_youtube_video_id(url)
                content = get_youtube_transcript(video_id)
            else:
                content = get_article_content(url)
        except Exception as e:
            return {
                "input_text": f"Error: {str(e)}",
                "is_valid": False,
                "messages": "ì½˜í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            }

    # 3. ì¶”ì¶œëœ ë‚´ìš©ì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° ë°©ì–´
    if not content:
        return {"is_valid": False, "messages": "ë¶„ì„í•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤."}

    # 4. Safety Check (LLMí™œìš©)
    try:
        check_text = content[:2000]
        safety_llm = llm.invoke(SAFETY_PROMPT + "\n\n[CONTENT]\n" + check_text)
        safety_response = (safety_llm.content or "").strip().upper()

        if "UNSAFE" in safety_response:
            return {
                "input_text": "Error: ìœ í•´ ì½˜í…ì¸  ê°ì§€",
                "is_valid": False,
                "is_safe": False,
                "messages": "ì•ˆì „í•˜ì§€ ì•Šì€ ì½˜í…ì¸ ë¡œ íŒë‹¨ë˜ì–´ ì¤‘ë‹¨í•©ë‹ˆë‹¤."
            }

        # ì„±ê³µì ìœ¼ë¡œ í†µê³¼í•œ ê²½ìš° ë¦¬í„´ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê¶Œì¥)
        return {
            "input_text": content,
            "is_valid": True,
            "is_safe": True,
            "messages": "ì½˜í…ì¸  ì¶”ì¶œ ë° ì•ˆì „ì„± ê²€ì‚¬ ì™„ë£Œ!"
        }

    except Exception as e:
        return {"is_valid": False, "is_safe": False, "messages": f"Safety Check ì—ëŸ¬: {str(e)}"}

def classify_node(state):
    """3) ì½˜í…ì¸  ì„±ê²©ì„ ë¶„ì„í•˜ì—¬ 'ì§€ì‹í˜•' ë˜ëŠ” 'íë§í˜•'ìœ¼ë¡œ ë¶„ë¥˜ (CoT ì ìš©)"""
    print("\n[Node] classify_node: ì½˜í…ì¸  ë¶„ë¥˜ ì¤‘...")
    article = state["input_text"]
    resp = llm.invoke(CLASSIFY_PROMPT + "\n\n[CONTENT]\n" + article[:2000])
    raw_output = (resp.content or "").strip()
    
    # "Category: [ì§€ì‹í˜•]" ë˜ëŠ” "Category: [íë§í˜•]"ì—ì„œ ì¶”ì¶œ
    if "ì§€ì‹í˜•" in raw_output:
        category = "ì§€ì‹í˜•"
    elif "íë§í˜•" in raw_output:
        category = "íë§í˜•"
    else:
        category = "ì§€ì‹í˜•"
        
    state["category"] = category
    return state


def synthesize_node(state):
    """4) ê¸°ì‚¬ ì›ë¬¸ìœ¼ë¡œ ìš”ì•½ ì´ˆì•ˆ(draft_summary)ë§Œ ìƒì„± (RAG ì‚¬ìš© X)"""
    print("[Node] synthesize_node: ìš”ì•½ ì´ˆì•ˆ ìƒì„± ì¤‘...")
    article = state["input_text"]

    resp = llm.invoke(SUMMARY_DRAFT_PROMPT + "\n\n[ARTICLE]\n" + article)
    draft = (resp.content or "").strip()

    state["draft_summary"] = draft
    return state


def verify_node(state):
    """5) ìš”ì•½ ì´ˆì•ˆì„ RAGë¡œ ê²€ì¦(ê·¼ê±° ë¬¸ë§¥ êµ¬ì„±/ë¬¸ì¥ ê²€ì¦ ê²°ê³¼ ì €ì¥)"""
    print("[Node] verify_node: RAG ê²€ì¦ ë° ë²¡í„° DB ìƒì„± ì¤‘ (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)...")
    article = state["input_text"]
    draft = state.get("draft_summary", "")

    # rag.pyì˜ ì›ë³¸ verify_summary_with_rag í˜¸ì¶œ (ì‹œê·¸ë‹ˆì²˜ì— ë§ì¶° ì§ì ‘ ì „ë‹¬)
    verified = verify_summary_with_rag(
        llm=llm,
        article_text=article,
        summary_draft=draft,
        per_sentence_k=3,
        relevance_threshold=0.12,
        max_context_chars=2800
    )

    state["query"] = verified.get("query", "")
    state["context"] = verified.get("context", "")
    state["citations"] = verified.get("citations", [])
    state["unsupported_sentences"] = verified.get("unsupported_sentences", [])

    verified_summary = verified.get("verified_summary", "")

    # ğŸ”§ ê³µë°± ì •ë¦¬ (ì´ìƒí•œ ì´ì¤‘ ê³µë°± ì œê±°)
    verified_summary = re.sub(r"\s+", " ", verified_summary).strip()

    state["summary"] = json.dumps(
        {
            "Summary": verified_summary,
            "UsedCitations": verified.get("used_citations", []),
            "Citations": verified.get("citations", []),
        },
        ensure_ascii=False,
    )

    # ì»¨í…ìŠ¤íŠ¸ê°€ ë¹„ì—ˆê±°ë‚˜ unsupportedê°€ ìˆìœ¼ë©´ ê°œì„  ë£¨í”„
    state["needs_improve"] = (not str(state["context"]).strip()) or (len(state["unsupported_sentences"]) > 0)

    return state


def judge_node(state):
    """6) ê²€ì¦ëœ CONTEXT vs SUMMARY faithfulness ì±„ì """
    context = state.get("context", "")
    summary_json = state.get("summary", "")

    try:
        s_obj = json.loads(summary_json)
        summary_text = s_obj.get("Summary", "")
    except Exception:
        summary_text = str(summary_json)

    resp = llm.invoke(
        JUDGE_PROMPT
        + "\n\n[CONTEXT]\n"
        + str(context)
        + "\n\n[SUMMARY]\n"
        + str(summary_text)
    )

    try:
        parsed = json.loads(resp.content)
    except Exception:
        parsed = {"score": 0, "needs_improve": True, "notes": "ì±„ì  JSON íŒŒì‹± ì‹¤íŒ¨"}

    score = int(parsed.get("score", 0))
    needs_improve = bool(parsed.get("needs_improve", score < 7))

    if state.get("unsupported_sentences"):
        needs_improve = True
        score = min(score, 6)

    state["judge_score"] = score
    state["needs_improve"] = needs_improve
    return state


def improve_node(state):
    """7) CONTEXT ê¸°ë°˜ìœ¼ë¡œ draft_summary(ì´ˆì•ˆ) ê°œì„ """
    max_improve = int(state.get("max_improve", 2))
    count = int(state.get("improve_count", 0))

    if count >= max_improve:
        state["needs_improve"] = False
        return state

    context = state.get("context", "")
    draft = state.get("draft_summary", "")

    resp = llm.invoke(
        IMPROVE_DRAFT_PROMPT
        + "\n\n[CONTEXT]\n"
        + str(context)
        + "\n\n[SUMMARY_DRAFT]\n"
        + str(draft)
    )

    improved_draft = (resp.content or "").strip()
    state["draft_summary"] = improved_draft
    state["improve_count"] = count + 1

    return state


def knowledge_augmentation_node(state: Dict[str, Any]):
    """
    ì§€ì‹í˜• ì½˜í…ì¸ ì— ëŒ€í•´ ì¶”ê°€ ì •ë³´ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤. (Tool-calling ë°©ì‹)
    1. ìµœì‹  ì •ë³´í˜• (Dynamic): get_latest_update_analysis ë„êµ¬ ìë™ í˜¸ì¶œ
    2. ê³ ì • ì§€ì‹í˜• (Static): ê°œì¸ URL DBì—ì„œ ë¹„ìŠ·í•œ ì •ë³´ ì¶”ì²œ
    """
    category = state.get("category", "ì§€ì‹í˜•")
    
    # íë§í˜•ì€ ë³´ê°• ì—†ì´ í†µê³¼
    if category != "ì§€ì‹í˜•":
        return state
        
    summary_json = state.get("summary", "")
    try:
        s_obj = json.loads(summary_json)
        summary_text = s_obj.get("Summary", "")
    except Exception:
        summary_text = str(summary_json)
    
    # ë„êµ¬ê°€ ë°”ì¸ë”©ëœ LLM ìƒì„±
    llm_with_tools = llm.bind_tools([get_latest_update_analysis])
    
    # 1. ì •ë³´ ìœ í˜• ë¶„ì„ ë° ë„êµ¬ í˜¸ì¶œ íŒë‹¨
    print("ğŸ§  ì½˜í…ì¸  ìœ í˜• ë¶„ì„ ë° ì›¹ ê²€ìƒ‰ ì—¬ë¶€ íŒë‹¨ ì¤‘...")
    resp = llm_with_tools.invoke([
        ("system", KNOWLEDGE_TYPE_CLASSIFY_PROMPT),
        ("human", f"ì´ ìš”ì•½ë³¸ì— ëŒ€í•´ ìµœì‹  ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í• ê¹Œ? í•„ìš”í•˜ë©´ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ê³ , ì•„ë‹ˆë©´ 'Static'ì´ë¼ê³  ë‹µí•´.\n\n[SUMMARY]\n{summary_text}")
    ])
    
    augmentation_info = ""
    
    # 2-1. LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•œ ê²½ìš° (Dynamic)
    if resp.tool_calls:
        print(f"ğŸ” [Dynamic] ìµœì‹  ì •ë³´ ì—…ë°ì´íŠ¸ í•„ìš”: {resp.tool_calls[0]['name']} ì‹¤í–‰ ì¤‘...")
        for tool_call in resp.tool_calls:
            if tool_call["name"] == "get_latest_update_analysis":
                # ë„êµ¬ ì‹¤í–‰ ë° ê²°ê³¼ íšë“
                result = get_latest_update_analysis.invoke(tool_call["args"])
                augmentation_info = "\n\n" + str(result)
                print("âœ… ì›¹ ê²€ìƒ‰ ë° ë¶„ì„ ì™„ë£Œ.")
    
    # 2-2. ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ê²½ìš° (Static ë“±)
    else:
        print("ğŸ“š [Static] ê³ ì • ì§€ì‹í˜• ì½˜í…ì¸ : ê´€ë ¨ ì½˜í…ì¸  ì¶”ì²œ ì§„í–‰...")
        try:
            db = get_db()
            recommends = db.get_similar_recommendations(category="ì§€ì‹í˜•", limit=2)
            if recommends:
                info_list = []
                for rec in recommends:
                    info_list.append(f"- {rec['url']} (í˜ë¥´ì†Œë‚˜: {rec['persona_style']})")
                augmentation_info = "\n\n[í•¨ê»˜ ë³´ë©´ ì¢‹ì€ ì½˜í…ì¸ ]\n" + "\n".join(info_list)
            else:
                augmentation_info = "\n\n[í•¨ê»˜ ë³´ë©´ ì¢‹ì€ ì½˜í…ì¸ ]\nì•„ì§ ì €ì¥ëœ ë¹„ìŠ·í•œ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            augmentation_info = f"\n\n(ì¶”ì²œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)})"
            
    state["augmentation_info"] = augmentation_info
    return state


def quiz_node(state):
    """(ì˜µì…˜) ìµœì¢… verified summary ê¸°ë°˜ í€´ì¦ˆ ë° ìƒê°ìœ ë„ì§ˆë¬¸ ìƒì„±"""
    category = state.get("category", "ì§€ì‹í˜•")

    # -----------------------------
    # 1ï¸âƒ£ Summary ì¶”ì¶œ
    # -----------------------------
    try:
        s_obj = json.loads(state.get("summary", ""))
        summary_text = s_obj.get("Summary", "")
    except Exception:
        summary_text = ""

    # ğŸ”¥ í€´ì¦ˆ ìƒì„±ìš©ì—ì„œëŠ” citation íƒœê·¸ ì œê±°
    summary_text = re.sub(r"\s*\[C\d+\]\s*", " ", summary_text).strip()
    
    # ì´ˆê¸°í™”
    state["thought_questions"] = []
    state["quiz"] = json.dumps({"questions": []}, ensure_ascii=False)

    # 1. ì§€ì‹í˜•: í€´ì¦ˆë§Œ ìƒì„±
    if category == "ì§€ì‹í˜•":
        resp_quiz = llm.invoke(QUIZ_FROM_SUMMARY_PROMPT + "\n\n[SUMMARY]\n" + str(summary_text))
        try:
            quiz_obj = json.loads(resp_quiz.content)
            if isinstance(quiz_obj, dict) and "questions" in quiz_obj:
                state["quiz"] = json.dumps(quiz_obj, ensure_ascii=False)
        except Exception:
            pass
    
    # 2. íë§í˜•: ìƒê° ìœ ë„ ì§ˆë¬¸ë§Œ ìƒì„±
    else:
        resp_thought = llm.invoke(
            THOUGHT_QUESTION_PROMPT 
            + f"\n\n[CATEGORY]: {category}"
            + "\n\n[SUMMARY]\n" + str(summary_text)
        )
        try:
            thought_questions = json.loads(resp_thought.content)
            state["thought_questions"] = thought_questions if isinstance(thought_questions, list) else []
        except Exception:
            pass

    return state



# ============================================================
# í˜ë¥´ì†Œë‚˜ ì ìš© ë…¸ë“œ
# ============================================================

def persona_node(state):
    """
    í™•ì •ëœ ìš”ì•½ê³¼ í€´ì¦ˆ/ì§ˆë¬¸ì— í˜ë¥´ì†Œë‚˜ë¥¼ ì…í™ë‹ˆë‹¤.
    
    ë™ì‘:
    1. í˜„ì¬ í˜ë¥´ì†Œë‚˜ ì¹´ìš´í„°ë¥¼ í™•ì¸ (0-9 ìˆœí™˜)
    2. ì½˜í…ì¸  ìœ í˜•ì— ë”°ë¼ í€´ì¦ˆí˜•/ë¬¸ì¥í˜• í˜ë¥´ì†Œë‚˜ ì„ íƒ
    3. í˜ë¥´ì†Œë‚˜ ìŠ¤íƒ€ì¼ì„ ì ìš©í•œ ë©”ì‹œì§€ ìƒì„±
    
    ì´ìœ :
    - ë§¤ë²ˆ ê°™ì€ ë§íˆ¬ë¡œ ì•Œë¦¼ì´ ì˜¤ë©´ ì‚¬ìš©ìê°€ ì§€ë£¨í•´ì ¸ ì•Œë¦¼ì„ ì°¨ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    - 10ê°€ì§€ í˜ë¥´ì†Œë‚˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ì—¬ 'ì¹œêµ¬ê°€ ì•ˆë¶€ë¥¼ ë¬»ëŠ”' ëŠë‚Œì„ ì¤ë‹ˆë‹¤.
    """
    category = state.get("category", "ì§€ì‹í˜•")
    persona_count = int(state.get("persona_count", 0))
    
    # í˜ë¥´ì†Œë‚˜ ì„ íƒ (0-9 ìˆœí™˜)
    if category == "ì§€ì‹í˜•":
        persona_key = f"quiz_{persona_count % 5}"
    else:
        persona_key = f"thought_{persona_count % 5}"
    
    persona_def = PERSONA_DEFINITIONS.get(persona_key, PERSONA_DEFINITIONS["quiz_0"])
    
    # ì ìš©í•  ì½˜í…ì¸  ì¤€ë¹„
    try:
        s_obj = json.loads(state.get("summary", ""))
        summary_text = s_obj.get("Summary", "")
    except Exception:
        summary_text = state.get("summary", "")
    
    if category == "ì§€ì‹í˜•":
        quiz_text = state.get("quiz", "")
        aug_info = state.get("augmentation_info", "")
        content_to_style = f"[ìš”ì•½]\n{summary_text}\n\n[í€´ì¦ˆ]\n{quiz_text}"
        if aug_info:
            content_to_style += f"\n\n{aug_info}"
    else:
        thought_text = "\n".join(state.get("thought_questions", []))
        content_to_style = f"[ìš”ì•½]\n{summary_text}\n\n[ìƒê° ìœ ë„ ì§ˆë¬¸]\n{thought_text}"
    
    # í˜ë¥´ì†Œë‚˜ ì ìš©
    prompt = PERSONA_APPLY_PROMPT.format(
        persona_definition=json.dumps(persona_def, ensure_ascii=False),
        content=content_to_style
    )
    
    resp = llm.invoke(prompt)
    styled_content = (resp.content or "").strip()
    
    # ìƒíƒœ ì—…ë°ì´íŠ¸
    state["persona_style"] = persona_def["name"]
    state["styled_content"] = styled_content
    state["persona_count"] = persona_count + 1
    
    return state


# ============================================================
# ì—ë¹™í•˜ìš°ìŠ¤ ìŠ¤ì¼€ì¤„ë§ ë…¸ë“œ
# ============================================================

def schedule_node(state):
    """
    ì—ë¹™í•˜ìš°ìŠ¤ ë§ê° ê³¡ì„ ì— ë”°ë¼ ë³µìŠµ ì•Œë¦¼ ë‚ ì§œë¥¼ ê³„ì‚°í•˜ê³  íŒì—… ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.
    
    ë™ì‘:
    1. ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ D+1, D+4, D+7, D+11 ê³„ì‚°
    2. ê³„ì‚°ëœ ë‚ ì§œë¥¼ ìƒíƒœì— ì €ì¥
    3. ë°ì´í„°ë² ì´ìŠ¤ì— ìŠ¤ì¼€ì¤„ ì˜êµ¬ ì €ì¥
    4. í¬ë¡œìŠ¤ í”Œë«í¼ íŒì—… ì•Œë¦¼ ë°œì†¡ (macOS + Windows)
    
    ì´ìœ : 
    - ì—ë¹™í•˜ìš°ìŠ¤ ë§ê° ê³¡ì„  ì´ë¡ :
      í•™ìŠµ ì§í›„ ë§ê°ì´ ê¸‰ê²©íˆ ì¼ì–´ë‚˜ì§€ë§Œ,
      ì ì ˆí•œ ì‹œì (1ì¼, 4ì¼, 7ì¼, 11ì¼)ì— ë³µìŠµí•˜ë©´
      ì •ë³´ê°€ ì¥ê¸° ê¸°ì–µìœ¼ë¡œ ì „í™˜ë©ë‹ˆë‹¤.
    - ë°œì†¡ ì‹œê°„: ì˜¤ì „ 8ì‹œ ì¶œê·¼ê¸¸ (ì¸ì§€ ë¶€í•˜ê°€ ì ì€ ì‹œê°„)
    - ì¼ì¼ ìµœëŒ€ 4íšŒ (ì•Œë¦¼ ìŠ¤íŠ¸ë ˆìŠ¤ ë°©ì§€ - ë“€ì˜¤ë§ê³  ë¬¸ì œì  ê°œì„ )
    - DB ì €ì¥: í”„ë¡œê·¸ë¨ ì¬ì‹œì‘ í›„ì—ë„ ìŠ¤ì¼€ì¤„ ìœ ì§€
    """
    schedule_dates = calculate_ebbinghaus_dates()
    state["schedule_dates"] = schedule_dates
    
    print(f"\nğŸ“… ì—ë¹™í•˜ìš°ìŠ¤ ì•Œë¦¼ ì˜ˆì•½ ì™„ë£Œ:")
    for i, date in enumerate(schedule_dates, 1):
        print(f"  {i}ì°¨ ì•Œë¦¼: {date} ì˜¤ì „ 8ì‹œ")
    
    # ğŸ†• ë°ì´í„°ë² ì´ìŠ¤ì— ìŠ¤ì¼€ì¤„ ì €ì¥
    schedule_id = None  # ì´ˆê¸°í™” (DB ì €ì¥ ì‹¤íŒ¨ ì‹œë¥¼ ëŒ€ë¹„)
    try:
        from agent.database import get_db
        
        db = get_db()
        
        # URL ì¶”ì¶œ (input_text ë˜ëŠ” ë³„ë„ url í•„ë“œ)
        url = state.get("url", "") or state.get("input_text", "")
        
        # ìš”ì•½ ì¶”ì¶œ (summaryëŠ” JSON ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ)
        summary_raw = state.get("summary", "")
        try:
            # JSON í˜•íƒœë©´ íŒŒì‹±
            summary_obj = json.loads(summary_raw)
            summary_text = summary_obj.get("Summary", str(summary_obj))
        except:
            summary_text = str(summary_raw)
        
        # í€´ì¦ˆ ë¬¸ì œ ì¶”ì¶œ (questionsëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ)
        questions = state.get("questions", [])
        
        schedule_id = db.save_schedule(
            user_id="default_user",  # í–¥í›„ ì‹¤ì œ ì‚¬ìš©ì IDë¡œ ëŒ€ì²´
            schedule_dates=schedule_dates,
            styled_content=state.get("styled_content", ""),
            persona_style=state.get("persona_style", ""),
            persona_count=state.get("persona_count", 0),
            url=url,
            summary=summary_text,
            category=state.get("category", "ì§€ì‹í˜•"),
            questions=questions  # âœ… í€´ì¦ˆ ë¬¸ì œ DBì— ì €ì¥
        )
        print(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ (Schedule ID: {schedule_id})")
        print(f"   - URL: {url[:50] if url else '(í…ìŠ¤íŠ¸ ì…ë ¥)'}...")
        print(f"   - ìš”ì•½: {summary_text[:50] if summary_text else '(ì—†ìŒ)'}...")
        print(f"   - í€´ì¦ˆ: {len(questions)}ê°œ ë¬¸ì œ ì €ì¥ë¨" if questions else "   - í€´ì¦ˆ: (ì—†ìŒ)")
    except Exception as e:
        print(f"\nâš ï¸  DB ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        print("   (ì•Œë¦¼ì€ ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤)")
    
    # ğŸ†• í¬ë¡œìŠ¤ í”Œë«í¼ íŒì—… ì•Œë¦¼ ë°œì†¡
    try:
        from agent.notification.popup import schedule_popup_notifications
        
        schedule_popup_notifications(
            schedule_dates=schedule_dates,
            styled_content=state.get("styled_content", ""),
            persona_style=state.get("persona_style", ""),
            category=state.get("category", "ì§€ì‹í˜•"),
            schedule_id=schedule_id  # âœ… DBì—ì„œ ìƒì„±ëœ ID ì „ë‹¬
        )
    except ImportError as e:
        print(f"\nâš ï¸  ì•Œë¦¼ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("   í•´ê²°: pip3 install plyer")
    except Exception as e:
        print(f"\nâš ï¸  ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
    
    return state
