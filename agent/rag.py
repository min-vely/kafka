import os
import json
import re
from typing import Any, Dict, List, Tuple, Optional

from langchain_upstage import UpstageEmbeddings, ChatUpstage
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever

# âœ… LangChain ë²„ì „ ì°¨ì´(íŒ¨í‚¤ì§€/ê²½ë¡œ) í˜¸í™˜ ì²˜ë¦¬
try:
    from langchain.text_splitter import CharacterTextSplitter  # êµ¬ë²„ì „
except Exception:
    from langchain_text_splitters import CharacterTextSplitter  # ìµœì‹  ë¶„ë¦¬ íŒ¨í‚¤ì§€

from agent.prompts import QUERY_REWRITE_PROMPT, RERANK_PROMPT


# -----------------------------
# 1) Vectorstore / Query Rewrite
# -----------------------------

try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except Exception:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

def build_vectorstore(text: str) -> FAISS:
    """ê¸°ì‚¬ ì›ë¬¸ì„ ì²­í¬ë¡œ ìª¼ê°œ ì„ë² ë”©í•œ ë’¤, FAISS ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=250,
        chunk_overlap=60,
        separators=["\n\n", "\n", ". ", "? ", "! ", " "],
    )
    chunks = splitter.split_text(text or "")

    embeddings = UpstageEmbeddings(
        model="solar-embedding-1-large",
        api_key=os.environ["UPSTAGE_API_KEY"],
    )
    return FAISS.from_texts(chunks, embeddings)




def rewrite_query(llm: ChatUpstage, article_text: str) -> str:
    """ê¸°ì‚¬ ì¼ë¶€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²€ìƒ‰ ìµœì í™” ì¿¼ë¦¬ë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤."""
    snippet = (article_text or "")[:1800]
    prompt = QUERY_REWRITE_PROMPT.strip() + "\n\n" + snippet

    resp = llm.invoke(prompt)

    content = ""
    try:
        content = (resp.content or "").strip()
    except Exception:
        content = str(resp).strip()

    # ë”°ì˜´í‘œ/ì¡ìŒ ì œê±°
    content = content.strip().strip('"').strip("'").strip()

    # ë¹ˆ ê°’ fallback
    if not content:
        content = "ê¸°ì‚¬ í•µì‹¬(ìˆ˜ì¹˜/ë¹„êµ/ê¸°ëŠ¥/ì¡°ê±´/ë°œì–¸)ì„ ìš”ì•½í•˜ê¸° ìœ„í•œ ê·¼ê±° ë¬¸ì¥ ê²€ìƒ‰ ì¿¼ë¦¬"

    # âœ… ì—¬ê¸°ì„œ ë°˜ë“œì‹œ ì •ë¦¬
    content = _clean_llm_query_output(content)

    return content



def _to_relevance(score: float) -> float:
    """FAISS scoreë¥¼ 0~1 relevanceë¡œ ë³€í™˜."""
    try:
        return 1.0 / (1.0 + float(score))
    except Exception:
        return 0.0


# âœ… rewrite_query ì¶œë ¥ì—ì„œ ë©”íƒ€ í…ìŠ¤íŠ¸/ë”°ì˜´í‘œ/ë§ˆí¬ë‹¤ìš´ ì œê±°
def _clean_llm_query_output(s: str, max_len: int = 160) -> str:
    s = (s or "").strip()

    # 1) ë¼ë²¨ ì œê±°
    s = re.sub(r"^\*+\s*ì¿¼ë¦¬\s*\*+\s*:\s*", "", s, flags=re.IGNORECASE)
    s = re.sub(r"^\s*query\s*:\s*", "", s, flags=re.IGNORECASE)

    # 2) í° ë¸”ë¡ì´ ì‹œì‘ë˜ëŠ” ì§€ì ì—ì„œ 'ì˜ë¼ë‚´ê¸°'
    #    (ì•„ë˜ í‚¤ì›Œë“œê°€ ë‚˜ì˜¤ë©´ ê·¸ ì´ì „ê¹Œì§€ë§Œ ë‚¨ê¹€)
    cut_markers = [
        "\n",                # ì—¬ëŸ¬ ì¤„ì´ë©´ ì²« ì¤„ë§Œ
        "citations:",        # citations ë¸”ë¡ ì œê±°
        "**ìµœì¢…",            # **ìµœì¢… ì¶œë ¥**, **ìµœì¢… ë‹µë³€** ì œê±°
        "ìµœì¢… ì¶œë ¥",         # í•œê¸€ ìµœì¢… ì¶œë ¥
        "ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­",    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜... ì œê±°
        "(â€»",                # ì£¼ì„/ì„¤ëª… ì œê±°
    ]
    for m in cut_markers:
        if m in s:
            s = s.split(m, 1)[0].strip()

    # 3) ë”°ì˜´í‘œë¡œ ì‹œì‘í•˜ëŠ” 'ë‘ ë²ˆì§¸ ë©ì–´ë¦¬'ê°€ ë¶™ëŠ” ê²½ìš°(ë„ˆ ë¡œê·¸ ì¼€ì´ìŠ¤) ì˜ë¼ë‚´ê¸°
    if ' "' in s:
        s = s.split(' "', 1)[0].strip()
    if '"' in s and s.count('"') >= 1:
        # ì¤‘ê°„ì— ë”°ì˜´í‘œê°€ ë‚˜ì˜¤ë©´ ë’¤ëŠ” ë²„ë¦¬ê¸° (LLMì´ "..." ë¸”ë¡ì„ ë¶™ì¼ ë•Œ)
        s = s.split('"', 1)[0].strip()

    # 4) ë©”íƒ€ ë¬¸êµ¬ ì œê±° (ë‚¨ì•„ ìˆìœ¼ë©´)
    s = re.sub(r"\*\*ìµœì¢…\s*ë‹µë³€\*\*|ìµœì¢…\s*ë‹µë³€|ì‹¤ì œ\s*ë‹µë³€", "", s).strip()

    # 5) ê³µë°±/ë”°ì˜´í‘œ ì •ë¦¬
    s = s.strip().strip('"').strip("'")
    s = re.sub(r"\s+", " ", s).strip()

    # 6) ê¸¸ì´ ì œí•œ
    if len(s) > max_len:
        s = s[:max_len].rstrip(" ,.;")

    return s



# -----------------------------
# 2) Retriever (optional rerank)
# -----------------------------
def retrieve_candidates(vs: FAISS, query: str, k: int = 8) -> List[Dict[str, Any]]:
    pairs = vs.similarity_search_with_score(query, k=k)
    cands: List[Dict[str, Any]] = []
    for idx, (doc, score) in enumerate(pairs, start=1):
        cid = f"C{idx}"
        cands.append(
            {
                "id": cid,
                "text": doc.page_content,
                "score": float(score),
                "relevance": _to_relevance(score),
            }
        )
    return cands


def rerank_with_llm(llm: ChatUpstage, query: str, candidates: List[Dict[str, Any]], take: int = 4) -> List[Dict[str, Any]]:
    """
    LLMìœ¼ë¡œ í›„ë³´ë¥¼ ì¬ì •ë ¬í•©ë‹ˆë‹¤.
    RERANK_PROMPTëŠ” ["C3","C1",...] ê°™ì€ id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ì•¼ í•¨.
    ì‹¤íŒ¨ ì‹œ relevance ìˆœìœ¼ë¡œ fallback.
    """
    payload = {
        "query": query,
        "candidates": [{"id": c["id"], "text": (c["text"][:400] if c.get("text") else "")} for c in candidates],
    }

    resp = llm.invoke(RERANK_PROMPT + "\n\n" + json.dumps(payload, ensure_ascii=False))

    picked_ids: List[str] = []
    try:
        picked = json.loads(resp.content)
        if isinstance(picked, list):
            picked_ids = [x for x in picked if isinstance(x, str)]
    except Exception:
        picked_ids = []

    if not picked_ids:
        # fallback: relevance ë†’ì€ ìˆœ
        picked_ids = [c["id"] for c in sorted(candidates, key=lambda x: x["relevance"], reverse=True)[:take]]

    id2 = {c["id"]: c for c in candidates}
    ranked = [id2[i] for i in picked_ids if i in id2]
    return ranked[:take]


def pack_context(ranked: List[Dict[str, Any]], max_chars: int = 2800) -> Tuple[str, List[Dict[str, Any]]]:
    """
    ì„ ì •ëœ ê·¼ê±°ë¥¼ [C#] ë§ˆì»¤ì™€ í•¨ê»˜ í”„ë¡¬í”„íŠ¸ì— ë„£ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ í•©ì¹©ë‹ˆë‹¤.
    ë°˜í™˜: (context_str, citations=[{"id": "C1", "text": "..."}...])
    """
    seen = set()
    packed: List[Dict[str, Any]] = []
    total = 0

    for c in ranked:
        t = (c.get("text") or "").strip()
        if not t or t in seen:
            continue
        seen.add(t)

        piece = f"[{c['id']}] {t}"
        if total + len(piece) > max_chars:
            break

        packed.append({"id": c["id"], "text": t})
        total += len(piece)

    context = "\n\n".join([f"[{p['id']}] {p['text']}" for p in packed])
    return context, packed


class KafkaMiniRetriever(BaseRetriever):
    """í˜„ì¬ í”„ë¡œì íŠ¸ RAG ê²€ìƒ‰ ë¡œì§ì„ LangChain Retriever í˜•íƒœë¡œ ë˜í•‘."""
    model_config = {"arbitrary_types_allowed": True}

    vectorstore: FAISS
    llm: ChatUpstage
    top_k: int = 8
    relevance_threshold: float = 0.20
    rerank_top: int = 4

    def _get_relevant_documents(self, query: str) -> List[Document]:
        candidates = retrieve_candidates(self.vectorstore, query=query, k=self.top_k)
        filtered = [c for c in candidates if c["relevance"] >= self.relevance_threshold]
        if not filtered:
            return []

        ranked = rerank_with_llm(self.llm, query=query, candidates=filtered, take=self.rerank_top)

        docs: List[Document] = []
        for c in ranked:
            docs.append(
                Document(
                    page_content=c["text"],
                    metadata={
                        "cid": c["id"],
                        "score": c["score"],
                        "relevance": c["relevance"],
                    },
                )
            )
        return docs


# -----------------------------
# 3) Public: retrieve_context()
# -----------------------------
def retrieve_context(
    llm: ChatUpstage,
    article_text: str,
    top_k: int = 8,
    rerank_top: int = 4,
    relevance_threshold: float = 0.20,
    max_context_chars: int = 2800,
) -> Tuple[str, str, List[Dict[str, Any]]]:
    """
    ê¸°ì‚¬ â†’ (FAISS) â†’ ì¿¼ë¦¬ ì¬ì‘ì„± â†’ Retriever ê²€ìƒ‰ â†’ pack â†’ ë°˜í™˜
    ë°˜í™˜: (query, context, citations)
    """
    vs = build_vectorstore(article_text)
    query = rewrite_query(llm, article_text)

    # ë””ë²„ê·¸ (ê³µë°±/ì¤„ë°”ê¿ˆ í¬í•¨ í™•ì¸)

    retriever = KafkaMiniRetriever(
        vectorstore=vs,
        llm=llm,
        top_k=top_k,
        relevance_threshold=relevance_threshold,
        rerank_top=rerank_top,
    )

    try:
        docs = retriever.invoke(query)
    except Exception:
        docs = retriever.get_relevant_documents(query)

    ranked = [
        {
            "id": (d.metadata.get("cid") or "C?"),
            "text": d.page_content,
            "score": float(d.metadata.get("score", 0.0)),
            "relevance": float(d.metadata.get("relevance", 0.0)),
        }
        for d in (docs or [])
    ]

    if not ranked:
        return query, "", []

    context, citations = pack_context(ranked, max_chars=max_context_chars)
    return query, context, citations



# -----------------------------
# 4) Public: verify_summary_with_rag()
#    (nodes.pyê°€ ê¸°ëŒ€í•˜ëŠ” ë°˜í™˜ í˜•íƒœë¡œ ë§ì¶¤)
# -----------------------------
def _split_sentences_ko(text: str) -> List[str]:
    """
    ëŸ¬í”„ ë¬¸ì¥ ë¶„ë¦¬. (ë„ˆë¬´ ì™„ë²½í•  í•„ìš” ì—†ìŒ: ê²€ì¦/ê·¼ê±°ë¶€ì°©ìš©)
    """
    text = (text or "").strip()
    if not text:
        return []
    parts = re.split(r"(?<=[\.\?\!ã€‚ï¼ï¼Ÿ])\s+|\n+", text)
    return [p.strip() for p in parts if p.strip()]


def verify_summary_with_rag(
    llm: ChatUpstage,
    article_text: str,
    summary_draft: str,
    per_sentence_k: int = 3,
    top_k: int = 8,
    rerank_top: int = 4,
    relevance_threshold: float = 0.20,
    max_context_chars: int = 2800,
    **kwargs: Any,
) -> Dict[str, Any]:
    """
    ìš”ì•½ ê²€ì¦(RAG):
    - ìš”ì•½ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°¬
    - ê° ë¬¸ì¥ì„ ì¿¼ë¦¬ë¡œ FAISSì—ì„œ ê·¼ê±° ì²­í¬ ê²€ìƒ‰
    - ê·¼ê±°ê°€ ìˆëŠ” ë¬¸ì¥ì€ [C#]ë¥¼ ë¶™ì—¬ verified_summary ìƒì„±
    - ì „ì²´ context/citations/used_citations/unsupported_sentences ë°˜í™˜

    nodes.pyê°€ ê¸°ëŒ€í•˜ëŠ” í‚¤:
    - verified_summary
    - context
    - citations
    - used_citations
    - unsupported_sentences
    """
    vs = build_vectorstore(article_text)
    # ê´€ì¸¡/ë””ë²„ê¹…ìš©: ê¸°ì‚¬ ì „ì²´ ê¸°ë°˜ 1ë¬¸ì¥ ê²€ìƒ‰ ì¿¼ë¦¬(verify ë£¨í”„ì—ì„œ ì§ì ‘ ì‚¬ìš©í•˜ì§„ ì•ŠìŒ)
    global_query = rewrite_query(llm, article_text)


    # ğŸ”§ ìˆ˜ì • ì‚¬í•­/ì£¼ì„ ë¸”ë¡ ì œê±° (ìµœì¢… ìš”ì•½ë§Œ ê²€ì¦)
    summary_draft = (summary_draft or "").split("â€» ìˆ˜ì • ì‚¬í•­:")[0].strip()
    #

    sentences = _split_sentences_ko(summary_draft)
    if not sentences:
        return {
            "query": global_query,
            "verified_summary": "",
            "context": "",
            "citations": [],
            "used_citations": [],
            "unsupported_sentences": [],
        }

    # ì „ì—­ ê·¼ê±° í’€ (í…ìŠ¤íŠ¸ ì¤‘ë³µ ë°©ì§€, C# ì¬ì‚¬ìš©)
    cite_text_to_id: Dict[str, str] = {}
    citations: List[Dict[str, Any]] = []
    used_citations: List[str] = []
    unsupported_sentences: List[str] = []

    def _get_or_make_cid(text: str) -> str:
        t = text.strip()
        if t in cite_text_to_id:
            return cite_text_to_id[t]
        cid = f"C{len(citations) + 1}"
        cite_text_to_id[t] = cid
        citations.append({"id": cid, "text": t})
        return cid

    # ë¬¸ì¥ë³„ ê·¼ê±° ì°¾ê¸°
    verified_lines: List[str] = []
    for sent in sentences:
        # 1) í›„ë³´ ê²€ìƒ‰
        cands = retrieve_candidates(vs, query=sent, k=max(top_k, per_sentence_k))
        filtered = [c for c in cands if c["relevance"] >= relevance_threshold]

        if not filtered:
            unsupported_sentences.append(sent)
            verified_lines.append(sent)
            continue

        # 2) rerank (ê°€ëŠ¥í•˜ë©´) â†’ per_sentence_kë§Œí¼ ì„ íƒ
        try:
            ranked = rerank_with_llm(llm, query=sent, candidates=filtered, take=max(per_sentence_k, 1))
        except Exception:
            ranked = sorted(filtered, key=lambda x: x["relevance"], reverse=True)[: max(per_sentence_k, 1)]

        # 3) citations ë“±ë¡ + ë¬¸ì¥ì— ë¶€ì°©
        #    - ì¤‘ë³µ chunkë¡œ ì ë¦¼ì„ ì¤„ì´ê¸° ìœ„í•´ 'ì„œë¡œ ë‹¤ë¥¸ í…ìŠ¤íŠ¸'ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì±„íƒ
        #    - per_sentence_kë§Œí¼ ëª» ì±„ìš°ë©´ thresholdë¥¼ ë‚®ì¶°(ì™„í™”) ì¶”ê°€ ì±„íƒ
        cids: List[str] = []

        def _add_unique_from(rows: List[Dict[str, Any]], limit: int, relax_factor: float = 1.0):
            nonlocal cids
            if not rows:
                return
            min_rel = relevance_threshold * relax_factor
            for r in rows:
                if len(cids) >= limit:
                    break
                if float(r.get("relevance", 0.0)) < min_rel:
                    continue
                t = (r.get("text") or "").strip()
                if not t:
                    continue
                cid = _get_or_make_cid(t)
                # ê°™ì€ citation idê°€ ë°˜ë³µ ë¶€ì°©ë˜ëŠ” ê±´ í—ˆìš©í•˜ë˜, í•œ ë¬¸ì¥ ë‚´ì—ì„œëŠ” ì¤‘ë³µ ì œê±°
                if cid not in cids:
                    cids.append(cid)

        # 1ì°¨: rerank ê²°ê³¼ì—ì„œ ìš°ì„  ì±„íƒ
        _add_unique_from(ranked, per_sentence_k, relax_factor=1.0)

        # 2ì°¨: ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ (rerank ì „ì—) filtered í›„ë³´ì—ì„œ ë‹¤ì–‘ì„± ë³´ì¶© (threshold ì™„í™”)
        if len(cids) < max(1, per_sentence_k):
            # relevance ë‚´ë¦¼ì°¨ìˆœ
            backup = sorted(filtered, key=lambda x: x["relevance"], reverse=True)
            _add_unique_from(backup, per_sentence_k, relax_factor=0.6)

        if not cids:
            unsupported_sentences.append(sent)
            verified_lines.append(sent)
            continue

        # used_citations ëˆ„ì 
        for cid in cids:
            if cid not in used_citations:
                used_citations.append(cid)

        # ë¬¸ì¥ ë’¤ì— [C#] ë¶€ì°©
        verified_lines.append(sent + " " + " ".join([f"[{cid}]" for cid in cids]))

    # ì „ì²´ context ë§Œë“¤ê¸° (max_context_chars ì œí•œ)
    context_blocks: List[str] = []
    total = 0
    for c in citations:
        block = f"[{c['id']}] {c['text']}"
        if total + len(block) > max_context_chars:
            break
        context_blocks.append(block)
        total += len(block)

    context = "\n\n".join(context_blocks)
    verified_summary = "\n".join(verified_lines).strip()


    return {
        "query": global_query,
        "verified_summary": verified_summary,
        "context": context,
        "citations": citations,
        "used_citations": used_citations,
        "unsupported_sentences": unsupported_sentences,
    }
