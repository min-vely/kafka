import os
import argparse
import json

from agent.graph import build_graph
from agent.utils import is_youtube_url, extract_youtube_video_id, get_youtube_transcript, get_article_content


def pretty_print(result: dict):
    print(f"\n========== CATEGORY: {result.get('category', 'N/A')} ==========")
    
    print("\n========== SUMMARY ==========")
    try:
        s = json.loads(result.get("summary", "{}"))
        print(s.get("Summary", s))
    except Exception:
        print(result.get("summary", ""))

    print("\n========== THOUGHT QUESTIONS ==========")
    tq = result.get("thought_questions", [])
    if tq:
        for i, q in enumerate(tq, 1):
            print(f"{i}. {q}")
    else:
        print("(no thought questions)")

    if result.get("category") == "ì§€ì‹í˜•":
        print("\n========== QUIZ ==========")
        try:
            q = json.loads(result.get("quiz", "{}"))
            questions = q.get("questions", [])
            if not questions:
                print("(no quiz items)")
            for i, item in enumerate(questions, 1):
                print(f"\nQ{i}. {item.get('text') or item.get('question')}")
                for opt in item.get("options", []):
                    print(f"  {opt}")
                print("  ì •ë‹µ:", item.get("answer"))
        except Exception:
            print(result.get("quiz", ""))
    
    print("\n========== JUDGE ==========")
    print("score:", result.get("judge_score"))
    print("needs_improve:", result.get("needs_improve"))
    print("improve_count:", result.get("improve_count", 0))

    # ğŸ†• í˜ë¥´ì†Œë‚˜ ì •ë³´ ì¶œë ¥
    print("\n========== PERSONA ==========")
    print("style:", result.get("persona_style", "N/A"))
    print("count:", result.get("persona_count", 0))
    
    # ğŸ†• í˜ë¥´ì†Œë‚˜ê°€ ì ìš©ëœ ìµœì¢… ë©”ì‹œì§€ ì¶œë ¥
    styled = result.get("styled_content", "")
    if styled:
        print("\n========== STYLED CONTENT (í˜ë¥´ì†Œë‚˜ ì ìš©) ==========")
        print(styled)
    
    # ğŸ†• ì—ë¹™í•˜ìš°ìŠ¤ ìŠ¤ì¼€ì¤„ ì¶œë ¥
    print("\n========== EBBINGHAUS SCHEDULE ==========")
    schedule_dates = result.get("schedule_dates", [])
    if schedule_dates:
        for i, date in enumerate(schedule_dates, 1):
            print(f"{i}ì°¨ ì•Œë¦¼: {date} ì˜¤ì „ 8ì‹œ (ì¶œê·¼ê¸¸)")
    else:
        print("(no schedule)")

    print("\n========== RAG ==========")
    print("query:", result.get("query", ""))
    cits = result.get("citations", [])
    if cits:
        print("citations:")
        for c in cits:
            cid = c.get("id")
            txt = (c.get("text") or "").replace("\n"," ")
            snip = (txt[:140] + "â€¦") if len(txt) > 140 else txt
            print(f" - {cid}: {snip}")
    else:
        print("citations: (none)")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--text", type=str, help="Input text")
    parser.add_argument("--url", type=str, help="YouTube URL or News Article URL")
    args = parser.parse_args()

    input_text = ""
    target_url = args.url
    raw_text = args.text

    # ì¸ìê°€ ì•„ë¬´ê²ƒë„ ì—†ì„ ê²½ìš° ëŒ€í™”í˜• ì…ë ¥ ëª¨ë“œ ì§„ì…
    if not target_url and not raw_text:
        user_input = input("URL ë˜ëŠ” í…ìŠ¤íŠ¸(íŒŒì¼ëª…)ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not user_input:
            print("ì…ë ¥ê°’ì´ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return

        # URL íŒë³„
        if user_input.startswith(("http://", "https://")):
            target_url = user_input
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        elif os.path.isfile(user_input):
            print(f"íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤: {user_input}")
            with open(user_input, "r", encoding="utf-8") as f:
                input_text = f.read()
        # ê·¸ ì™¸ì—ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        else:
            input_text = user_input

    # URL ì²˜ë¦¬ ë¡œì§
    if target_url:
        if is_youtube_url(target_url):
            print(f"Extracting transcript from YouTube: {target_url}")
            video_id = extract_youtube_video_id(target_url)
            input_text = get_youtube_transcript(video_id)
        else:
            print(f"Extracting article content from: {target_url}")
            input_text = get_article_content(target_url)
    elif raw_text:
        input_text = raw_text

    if not input_text:
        print("ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    if not os.getenv("UPSTAGE_API_KEY"):
        raise ValueError("UPSTAGE_API_KEY not set")

    graph = build_graph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "input_text": input_text,
        "max_improve": 2
    }
    
    # URLì´ ìˆìœ¼ë©´ ì¶”ê°€
    if target_url:
        initial_state["url"] = target_url
    
    result = graph.invoke(initial_state)
    pretty_print(result)


if __name__ == "__main__":
    main()
